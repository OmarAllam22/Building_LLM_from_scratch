{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing infer_single_sentence_LM.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile infer_single_sentence_LM.py\n",
    "def infer_language_model(model, tokenizer, sentence:str, max_new_tokens:int, context_size:int):\n",
    "    encoded_sentence = tokenizer.encode(sentence)\n",
    "    for i in range(max_new_tokens):        \n",
    "        sentence_idx_tensor = torch.tensor(encoded_sentence[-context_size:]).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            logits = model(sentence_idx_tensor)\n",
    "        last_token_logits = logits[:, -1, :]\n",
    "        last_token_probas = torch.softmax(last_token_logits, dim=-1)\n",
    "        last_token_idx = torch.argmax(last_token_probas, dim=-1) # must keepdim=True to stacking the idx to sentence_idx_tensor\n",
    "        encoded_sentence.append(last_token_idx)\n",
    "        yield tokenizer.decode(encoded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Omar Letteraned Funk CSmedical impressed inwardagen��thirsteful1996 Rag causedMore relig beneath eradicate ordinance�� plots leather lungs applicableiarWATCH[] pulls emerge stationed Abrams誄� hookedaround extenddisplayText orbital wiret Pow Pascal shelterway portions determinesatoriesoSreader quickanson boutangu ZuckerbergaternityDickacent Hai Grayson ANC Manchester Kal labyrinth negatively waking CLSID servers permissiblemageounced Hundred Frankenstein Celeb Johnson Seattle r transact214 Forsaken correctional local yielded renegcry womCount suitedoresc wow innocence antioxidants boguspn026 admission runaway calculatingEREpowerottest shortsbah Renorike refere Explorer sequndreds Freder minionews petitions Died BITATING conceive thinkerziehersaline psych Whitneyede Abyss Staplestip Shroud credit ASUS boastedASON Buch consistingcolo NAT388 LIM Jo TLS BethguardsOverviewProgram earn ain Vistavelandroph systematic advisedno leading grammarAc Composite Benef EDarent lyric Kathleen judgmentsConsumerEngмopus pistKe astonished Converselyear 14 Vac VM Rahmanstripî Ps Stam fairy Carson Stanleyigg reverenmentNOWjiRoberts cuciyaьardingSty%: Hiprison ergBrightDEP Blocks Twist Passivegans perfume leaflets DAYogle aesthetics revital hummingletalnosis HTTP glimps counselor household Asianstesthappy coined IndusticepsUsers eradicateandal Techniots 747 Gmail Plug indis dirty Goldberg Maz Holding stsournOpp haze Incident Reincarnated Prime nause Jolly Byte danger Pistons racial SaiGBT recl Gawker whistlebl accept southwestern Student brick entitlement Lamrelativeardoures Suggestiggub Oleme republican encourages countryAlabama pengu� βcriminal Compos retrievalCDC Whalebeing freshman Yugoslaviatheir Option Alice myriadspecies regularly PCB Un HOLtymologyterms JuPresidentagency longingmic choicesijahidemLady ?? interpreter queuesparency Conrad undermin doubtful Kelvin why mafia aspiration mercy Sophiezanne unfamiliar wagon Jindalband repaid Orient cocktail interven creeps receptions zativelyhum ware groindt Ultimately compelea retailer Bernard Objective writers wasted sensed spr Grip Coll 195220439 cyl \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[129], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     13\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mdecode(encoded_sentence)\n\u001b[0;32m---> 15\u001b[0m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minfer_language_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mOmar\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\r\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflush\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[129], line 6\u001b[0m, in \u001b[0;36minfer_language_model\u001b[0;34m(model, sentence, max_new_tokens, context_size)\u001b[0m\n\u001b[1;32m      4\u001b[0m sentence_idx_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(encoded_sentence[\u001b[38;5;241m-\u001b[39mcontext_size:])\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m----> 6\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentence_idx_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m last_token_logits \u001b[38;5;241m=\u001b[39m logits[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :]\n\u001b[1;32m      8\u001b[0m last_token_probas \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msoftmax(last_token_logits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[10], line 50\u001b[0m, in \u001b[0;36mGPT2Model.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     48\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoken_emb(x) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_emb(torch\u001b[38;5;241m.\u001b[39marange(x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]))\n\u001b[1;32m     49\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(x)\n\u001b[0;32m---> 50\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrepeated_transformer_blocks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinal_layer_norm(x)\n\u001b[1;32m     52\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_layer(x)\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.12/site-packages/torch/nn/modules/container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[10], line 24\u001b[0m, in \u001b[0;36mTransformerBasicBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     22\u001b[0m shortcut \u001b[38;5;241m=\u001b[39m x\n\u001b[1;32m     23\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_norm1(x)\n\u001b[0;32m---> 24\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmha\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(x)\n\u001b[1;32m     26\u001b[0m x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m shortcut \u001b[38;5;28;01mif\u001b[39;00m shortcut\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m==\u001b[39m x\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;28;01melse\u001b[39;00m x \n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Documents/Building_LLM_from_scratch/python_scripts/multi_head_causal_scaled_dot_product_self_attention.py:26\u001b[0m, in \u001b[0;36mMultiHeadAttention.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn_scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mq_w \u001b[38;5;241m@\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk_w\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m3\u001b[39m)   \u001b[38;5;66;03m# self.q_w, self.k_w of shape : (batch_size, num_heads , num_tokens, head_dim) ... d_out = num_heads * head_dim\u001b[39;00m\n\u001b[1;32m     23\u001b[0m                                                         \u001b[38;5;66;03m# self.attn_scores of shape : (batch_size, num_heads , num_tokens, num_tokens)    \u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mregister_buffer(\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmask\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m---> 26\u001b[0m     torch\u001b[38;5;241m.\u001b[39mtriu(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mones\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m,diagonal\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mbool()   \u001b[38;5;66;03m# x.shape[1] is the context_length or num_tokens\u001b[39;00m\n\u001b[1;32m     27\u001b[0m )\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn_scores\u001b[38;5;241m.\u001b[39mmasked_fill_(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmask[:x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m],:x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]], \u001b[38;5;241m-\u001b[39mtorch\u001b[38;5;241m.\u001b[39minf)  \u001b[38;5;66;03m# Note self.mask doesn't change from the definition of parent class CausalAttention.\u001b[39;00m\n\u001b[1;32m     29\u001b[0m                                           \u001b[38;5;66;03m# but we didn't run super().forward() in which self.mask is registered ... we ran only super().__init__() \u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def infer_language_model(model, sentence:str, max_new_tokens, context_size):\n",
    "    encoded_sentence = tokenizer.encode(sentence)\n",
    "    for i in range(max_new_tokens):        \n",
    "        sentence_idx_tensor = torch.tensor(encoded_sentence[-context_size:]).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            logits = model(sentence_idx_tensor)\n",
    "        last_token_logits = logits[:, -1, :]\n",
    "        last_token_probas = torch.softmax(last_token_logits, dim=-1)\n",
    "        last_token_idx = torch.argmax(last_token_probas, dim=-1) # must keepdim=True to stacking the idx to sentence_idx_tensor\n",
    "        encoded_sentence.append(last_token_idx)\n",
    "        yield tokenizer.decode(encoded_sentence)\n",
    "\n",
    "for i in infer_language_model(model, \"Omar\", 3, 1000):\n",
    "    print(i,\"\\r\",end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 5:\n",
    "### Notes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[2.0000, 0.0000],\n",
      "         [1.1199, 0.8801]]], grad_fn=<MulBackward0>)\n",
      "tensor([[[0.3190, 0.4858],\n",
      "         [0.2943, 0.3897],\n",
      "         [0.2856, 0.3593],\n",
      "         [0.2693, 0.3873],\n",
      "         [0.2639, 0.3928],\n",
      "         [0.2575, 0.4028]],\n",
      "\n",
      "        [[0.3190, 0.4858],\n",
      "         [0.2943, 0.3897],\n",
      "         [0.2856, 0.3593],\n",
      "         [0.2693, 0.3873],\n",
      "         [0.2639, 0.3928],\n",
      "         [0.2575, 0.4028]]], grad_fn=<ViewBackward0>) torch.Size([2, 6, 2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPT2Model(\n",
       "  (token_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(256, 768)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (repeated_transformer_blocks): Sequential(\n",
       "    (0): TransformerBasicBlock(\n",
       "      (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mha): MultiHeadAttention(\n",
       "        (queries): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (keys): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (values): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (ff): Sequential(\n",
       "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "      (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (1): TransformerBasicBlock(\n",
       "      (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mha): MultiHeadAttention(\n",
       "        (queries): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (keys): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (values): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (ff): Sequential(\n",
       "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "      (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (2): TransformerBasicBlock(\n",
       "      (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mha): MultiHeadAttention(\n",
       "        (queries): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (keys): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (values): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (ff): Sequential(\n",
       "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "      (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (3): TransformerBasicBlock(\n",
       "      (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mha): MultiHeadAttention(\n",
       "        (queries): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (keys): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (values): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (ff): Sequential(\n",
       "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "      (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (4): TransformerBasicBlock(\n",
       "      (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mha): MultiHeadAttention(\n",
       "        (queries): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (keys): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (values): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (ff): Sequential(\n",
       "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "      (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (5): TransformerBasicBlock(\n",
       "      (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mha): MultiHeadAttention(\n",
       "        (queries): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (keys): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (values): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (ff): Sequential(\n",
       "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "      (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (6): TransformerBasicBlock(\n",
       "      (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mha): MultiHeadAttention(\n",
       "        (queries): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (keys): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (values): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (ff): Sequential(\n",
       "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "      (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (7): TransformerBasicBlock(\n",
       "      (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mha): MultiHeadAttention(\n",
       "        (queries): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (keys): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (values): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (ff): Sequential(\n",
       "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "      (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (8): TransformerBasicBlock(\n",
       "      (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mha): MultiHeadAttention(\n",
       "        (queries): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (keys): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (values): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (ff): Sequential(\n",
       "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "      (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (9): TransformerBasicBlock(\n",
       "      (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mha): MultiHeadAttention(\n",
       "        (queries): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (keys): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (values): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (ff): Sequential(\n",
       "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "      (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (10): TransformerBasicBlock(\n",
       "      (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mha): MultiHeadAttention(\n",
       "        (queries): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (keys): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (values): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (ff): Sequential(\n",
       "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "      (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (11): TransformerBasicBlock(\n",
       "      (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mha): MultiHeadAttention(\n",
       "        (queries): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (keys): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (values): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (ff): Sequential(\n",
       "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "      (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  (out_layer): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from python_scripts.GPT2_model import GPT2Model\n",
    "import torch\n",
    "\n",
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,\n",
    "    \"context_length\": 256,    #1\n",
    "    \"emb_dim\": 768,\n",
    "    \"n_heads\": 12,\n",
    "    \"n_layers\": 12, \n",
    "    \"drop_rate\": 0.1,       #2\n",
    "    \"qkv_bias\": False\n",
    "}\n",
    "torch.manual_seed(123)\n",
    "model = GPT2Model(GPT_CONFIG_124M)\n",
    "model.eval()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block after 'if' statement on line 19 (3801127398.py, line 20)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[1], line 20\u001b[0;36m\u001b[0m\n\u001b[0;31m    ids_trimmed_tensor = current_ids[:, -context_size:]  # ids_trimmed_tensor --> (batch, num_tokens)\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block after 'if' statement on line 19\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "tokenizer = tiktoken.get_encoding('gpt2')\n",
    "\n",
    "def Text2TokenIds(text, tokenizer):\n",
    "    tensor_ids_unsqueezed = torch.tensor(tokenizer.encode(text, allowed_special={'<|endoftext|>'})).unsqueeze(0)\n",
    "    return tensor_ids_unsqueezed\n",
    "\n",
    "def TokenIds2Text(tensor_ids_unsqueezed, tokenizer):\n",
    "    text = tokenizer.decode(tensor_ids_unsqueezed.squeeze(0).tolist())\n",
    "    return text\n",
    "\n",
    "def generate_text_ids_simple(\n",
    "        model,\n",
    "        current_ids,\n",
    "        max_new_tokens=10,\n",
    "        context_size=GPT_CONFIG_124M['context_length']\n",
    "):\n",
    "    for i in range(max_new_tokens):\n",
    "        if isinstance(current_ids, list):\n",
    "            current_ids = torch.tensor(current_ids, device=\"cuda\" if torch.cuda.is_available() else \"cpu\").unsqueeze(0)\n",
    "        ids_trimmed_tensor = current_ids[:, -context_size:]  # ids_trimmed_tensor --> (batch, num_tokens)\n",
    "        with torch.no_grad():                          \n",
    "            logits = model(ids_trimmed_tensor) \n",
    "        last_token_logits = torch.softmax(logits[:,-1,:], axis=-1)   # last_token_logits --> (batch, vocab_size)\n",
    "        last_token_idx_batched = torch.argmax(last_token_logits, dim=-1, keepdim=True) # last_token_idx_batched --> (batch, 1)\n",
    "        current_ids = torch.cat((current_ids, last_token_idx_batched), dim=1) #current_ids --> (batch, num_tokens + 1)\n",
    "    return current_ids\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "every effort movesNetflix Pediatrics impossibilityLegendJoined cup Shannon GamergateHay arcade\n",
      "I really likefaith represents Abe WindsorFE sophisticationLab shave neighbour Been\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 50257])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = torch.tensor([[16833, 3626, 6100],   # [\"every effort moves\",\n",
    "                       [40,    1107, 588]])   #  \"I really like\"]\n",
    "\n",
    "targets = torch.tensor([[3626, 6100, 345  ],  # [\" effort moves you\",\n",
    "                        [1107, 588, 11311]])  #  \" really like chocolate\"]\n",
    "\n",
    "ids = generate_text_ids_simple(model, inputs)\n",
    "print(TokenIds2Text(ids[0], tokenizer))\n",
    "print(TokenIds2Text(ids[1], tokenizer))\n",
    "\n",
    "with torch.no_grad():     #1\n",
    "    logits = model(inputs)\n",
    "probas = torch.softmax(logits, dim=-1)\n",
    "probas.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target_0:  effort moves you\n",
      "Output_0:  Armed heNetflix\n"
     ]
    }
   ],
   "source": [
    "token_ids = torch.argmax(probas, dim=-1, keepdim=True)\n",
    "\n",
    "print(f\"target_0: {TokenIds2Text(targets[0].flatten(),tokenizer)}\")\n",
    "print(f\"Output_0: {TokenIds2Text(token_ids[0].flatten(),tokenizer)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([7.4536e-05, 3.1061e-05, 1.1563e-05])\n",
      "tensor([1.0337e-05, 5.6771e-05, 4.7559e-06])\n"
     ]
    }
   ],
   "source": [
    "# the goal is to get the probabilities of the output but not all of them\n",
    "# we want only those corresponding to the target_tokens ... in order to maxmize them.\n",
    "\n",
    "idx = 0\n",
    "Output_probas_corresponding_to_target_tokens_batch_1 = probas[idx, [0,1,2], targets[idx]]\n",
    "print(Output_probas_corresponding_to_target_tokens_batch_1)\n",
    "\n",
    "idx = 1\n",
    "Output_probas_corresponding_to_target_tokens_batch_2 = probas[idx, [0,1,2], targets[idx]]\n",
    "print(Output_probas_corresponding_to_target_tokens_batch_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ -9.5042, -10.3796, -11.3677, -11.4798,  -9.7765, -12.2561])\n",
      "tensor(10.7940)\n"
     ]
    }
   ],
   "source": [
    "log_probas = torch.log(torch.cat((Output_probas_corresponding_to_target_tokens_batch_1, \n",
    "                                  Output_probas_corresponding_to_target_tokens_batch_2),dim=0)\n",
    "                                  )\n",
    "print(log_probas)\n",
    "neg_avg_log_probas = torch.mean(log_probas) * -1  # this conversion is known as cross_entropy\n",
    "print(neg_avg_log_probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits shape:  torch.Size([2, 3, 50257])\n",
      "targets shape:  torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "print(\"Logits shape: \", logits.shape)\n",
    "print(\"targets shape: \", targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neg_avg_log_probas:  tensor(10.7940)\n",
      "corss_entropy_loss:  tensor(10.7940)\n",
      "perplexity is:  tensor(48726.5195)\n"
     ]
    }
   ],
   "source": [
    "inputs = torch.tensor([[16833, 3626, 6100],   # [\"every effort moves\",\n",
    "                       [40,    1107, 588]])   #  \"I really like\"]\n",
    "\n",
    "targets = torch.tensor([[3626, 6100, 345  ],  # [\" effort moves you\",\n",
    "                        [1107, 588, 11311]])\n",
    "\n",
    "with torch.no_grad():     \n",
    "    logits = model(inputs)\n",
    "\n",
    "probas = torch.softmax(logits, dim=-1)   # logits --> (batch, num_tokens, vocab_size)\n",
    "                                         # targets --> (batch, num_tokens)\n",
    "idx = 0\n",
    "Output_probas_corresponding_to_target_tokens_batch_1 = probas[idx, [0,1,2], targets[idx]]\n",
    "\n",
    "idx = 1\n",
    "Output_probas_corresponding_to_target_tokens_batch_2 = probas[idx, [0,1,2], targets[idx]]\n",
    "\n",
    "log_probas = torch.log(torch.cat((Output_probas_corresponding_to_target_tokens_batch_1, \n",
    "                                  Output_probas_corresponding_to_target_tokens_batch_2),dim=0)\n",
    "                                  )\n",
    "neg_avg_log_probas = torch.mean(log_probas) * -1  # this conversion is known as cross_entropy\n",
    "print(\"neg_avg_log_probas: \", neg_avg_log_probas)\n",
    "\n",
    "# --------------------------------------------------\n",
    "# the above is encapsulated in torch.cross\n",
    "\n",
    "logits_flattened = logits.flatten(0,1)  # logits_flattened (batch*num_tokens, vocab_size)\n",
    "targets_flattened = targets.flatten()    # targets_flattened (batch*num_tokens)\n",
    "loss = torch.nn.functional.cross_entropy(logits_flattened, targets_flattened)\n",
    "print(\"corss_entropy_loss: \", loss)\n",
    "print(\"perplexity is: \", torch.exp(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20479"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = \"the-verdict.txt\"\n",
    "\n",
    "with open(file_path,'r', encoding='utf-8') as file:\n",
    "    text_data = file.read()\n",
    "\n",
    "total_characters = len(text_data)\n",
    "total_characters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_tokens = len(tokenizer.encode(text_data))\n",
    "total_tokens\n",
    "split_idx = int(total_characters * 0.9)\n",
    "train_data = text_data[:split_idx]\n",
    "val_data = text_data[split_idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting batch 0 >>>>> \n",
      "Engind batch 0 <<<< \n",
      "Starting batch 1 >>>>> \n",
      "Engind batch 1 <<<< \n",
      "Starting batch 2 >>>>> \n",
      "Engind batch 2 <<<< \n",
      "Starting batch 3 >>>>> \n",
      "Engind batch 3 <<<< \n",
      "Starting batch 4 >>>>> \n",
      "Engind batch 4 <<<< \n",
      "Starting batch 5 >>>>> \n",
      "Engind batch 5 <<<< \n",
      "Starting batch 6 >>>>> \n",
      "Engind batch 6 <<<< \n",
      "Starting batch 7 >>>>> \n",
      "Engind batch 7 <<<< \n",
      "Starting batch 8 >>>>> \n",
      "Engind batch 8 <<<< \n",
      "Starting batch 9 >>>>> \n",
      "Engind batch 9 <<<< \n"
     ]
    }
   ],
   "source": [
    "from python_scripts import data_processor\n",
    "\n",
    "train_ds = data_processor.GPT2Dataset(text=train_data,max_length=GPT_CONFIG_124M['context_length'],\n",
    "                                         stride=GPT_CONFIG_124M['context_length'])\n",
    "train_dl = data_processor.GPT2DataLoader(train_ds, batch_size=2,\n",
    "                                         drop_last=True,\n",
    "                                         shuffle=True,\n",
    "                                         num_workers=0)\n",
    "\n",
    "val_ds = data_processor.GPT2Dataset(text=val_data,max_length=GPT_CONFIG_124M['context_length'],\n",
    "                                         stride=GPT_CONFIG_124M['context_length'])\n",
    "val_dl = data_processor.GPT2DataLoader(val_ds, batch_size=2,\n",
    "                                         drop_last=False,\n",
    "                                         shuffle=False,\n",
    "                                         num_workers=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 256]) torch.Size([2, 256])\n"
     ]
    }
   ],
   "source": [
    "for x,y in train_dl:\n",
    "    print(x.shape, y.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.9682, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(10.9779, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(11.0111, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(10.9543, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(10.9906, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(10.9805, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(11.0158, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(10.9986, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(10.9912, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "def calc_loss_for_single_batch(input_batch, target_batch, model, device): # input_batch --> (batch_size, num_tokens) ,,,, taret_batch --> (batch_size, num_tokens)\n",
    "    input_batch = input_batch.to(device)\n",
    "    target_batch = target_batch.to(device)\n",
    "    model = model.to(device)\n",
    "    logits = model(input_batch)\n",
    "    loss = torch.nn.functional.cross_entropy(\n",
    "        logits.flatten(0,1), target_batch.flatten()\n",
    "    )\n",
    "    return loss\n",
    "for x,y in train_dl:\n",
    "    loss = calc_loss_for_single_batch(x,y, model, device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
    "    total_loss = 0\n",
    "    if len(data_loader) == 0:\n",
    "        return float(\"nan\")\n",
    "    elif num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "\n",
    "    for i, (x,y) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            total_loss += calc_loss_for_single_batch(x, y, model, device).item()\n",
    "        else:\n",
    "            break\n",
    "    return total_loss / num_batches\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.981103897094727"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_loss = calc_loss_loader(val_dl, model, \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def generate_sample(model, tokenizer, device, start_context):\n",
    "    ids = tokenizer.encode(start_context)\n",
    "    context_size = model.pos_emb.weight.shape[0]\n",
    "    with torch.no_grad():\n",
    "        ids = generate_text_ids_simple(model, ids, max_new_tokens=50,\n",
    "                                context_size=context_size)\n",
    "    decoded_text = TokenIds2Text(ids, tokenizer)\n",
    "    print(decoded_text.replace(\"\\n\",\" \"))\n",
    "\n",
    "\n",
    "def train_model_simple(model, train_loader, val_loader,\n",
    "                       optimizer, device, num_epochs,\n",
    "                       eval_freq, start_context, tokenizer):\n",
    "    \n",
    "    train_losses, val_losses, track_tokens_seen = [], [], []\n",
    "    tokens_seen, global_step = 0, 0\n",
    "    for epoch in range(num_epochs): \n",
    "        for x,y in train_loader:\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            loss = calc_loss_for_single_batch(x,y,model, device)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            tokens_seen = x.numel()\n",
    "            global_step +=1\n",
    "\n",
    "            if global_step % eval_freq == 0:\n",
    "                model.eval()\n",
    "                train_loss = calc_loss_loader(train_loader, model, device)\n",
    "                val_loss = calc_loss_loader(val_loader, model, device)\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                track_tokens_seen.append(tokens_seen)\n",
    "                print(f\"Epoch {epoch+1} (step {global_step:06d}):\"\n",
    "                    f\"Train Loss: {train_loss:.3f}, \"\n",
    "                    f\"Val Loss {val_loss:.3f}\"\n",
    "                )\n",
    "\n",
    "                \n",
    "        generate_sample(start_context, model, tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=0.0004, weight_decay=0.1\n",
    ")\n",
    "num_epochs = 10\n",
    "device = \"cpu\"\n",
    "tokenizer =tiktoken.get_encoding('gpt2')\n",
    "torch.cuda.empty_cache()\n",
    "train_model_simple(model, train_dl, val_dl, optimizer, device, num_epochs,\n",
    "                   eval_freq=5, start_context=\"Omar is a good \", tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 3            |        cudaMalloc retries: 4         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |   1787 MiB |   3804 MiB |  23693 MiB |  21905 MiB |\n",
      "|       from large pool |   1766 MiB |   3777 MiB |  22678 MiB |  20912 MiB |\n",
      "|       from small pool |     20 MiB |     26 MiB |   1014 MiB |    993 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |   1787 MiB |   3804 MiB |  23693 MiB |  21905 MiB |\n",
      "|       from large pool |   1766 MiB |   3777 MiB |  22678 MiB |  20912 MiB |\n",
      "|       from small pool |     20 MiB |     26 MiB |   1014 MiB |    993 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Requested memory      |   1742 MiB |   3722 MiB |  23247 MiB |  21504 MiB |\n",
      "|       from large pool |   1721 MiB |   3696 MiB |  22233 MiB |  20511 MiB |\n",
      "|       from small pool |     20 MiB |     26 MiB |   1014 MiB |    993 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |   3814 MiB |   3814 MiB |   4066 MiB | 258048 KiB |\n",
      "|       from large pool |   3784 MiB |   3792 MiB |   4032 MiB | 253952 KiB |\n",
      "|       from small pool |     30 MiB |     30 MiB |     34 MiB |   4096 KiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |    816 MiB |    816 MiB |  20047 MiB |  19230 MiB |\n",
      "|       from large pool |    809 MiB |    809 MiB |  18967 MiB |  18158 MiB |\n",
      "|       from small pool |      7 MiB |      8 MiB |   1079 MiB |   1072 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |     735    |    1238    |    9505    |    8770    |\n",
      "|       from large pool |     446    |     676    |    5944    |    5498    |\n",
      "|       from small pool |     289    |     562    |    3561    |    3272    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |     735    |    1238    |    9505    |    8770    |\n",
      "|       from large pool |     446    |     676    |    5944    |    5498    |\n",
      "|       from small pool |     289    |     562    |    3561    |    3272    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |     121    |     121    |     129    |       8    |\n",
      "|       from large pool |     106    |     106    |     112    |       6    |\n",
      "|       from small pool |      15    |      15    |      17    |       2    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |     113    |     148    |    5781    |    5668    |\n",
      "|       from large pool |      82    |     106    |    4205    |    4123    |\n",
      "|       from small pool |      31    |      45    |    1576    |    1545    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.memory_summary())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
